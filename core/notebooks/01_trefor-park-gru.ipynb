{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b125c3-7f05-45ba-8815-cd3ef899ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from core.util.io import read_csv\n",
    "from core.util.get_datasets import get_trefor_park_as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4685e80-cc1d-4e8d-b82d-c8c1454c83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "EPOCHS = 250\n",
    "lookback = 24\n",
    "loss_function = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac8f21-d4b6-486a-b614-629d4b34de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = read_csv(\"processed/park_training.csv\")\n",
    "data_val = read_csv(\"processed/park_validation.csv\")\n",
    "data_test = read_csv(\"processed/park_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9beea75-0d71-40bf-9e99-dfad13bc0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_trefor_park_as_tensor(data_train)\n",
    "x_val, y_val = get_trefor_park_as_tensor(data_val)\n",
    "x_test, y_test = get_trefor_park_as_tensor(data_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0eff-01ad-4f3a-a126-2e2922e2c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1624503-4387-4cce-8103-36f7a9ed9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreforData(Dataset):\n",
    "    \"\"\"Initialize Trefor dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x: torch.tensor, y: torch.tensor) -> None:\n",
    "        \"\"\"Initialize dataset.\n",
    "\n",
    "        Arguments:\n",
    "            x: feature as torch\n",
    "            y: target as torch\n",
    "\n",
    "        \"\"\"\n",
    "        self.x = x.to(device)\n",
    "        self.y = y.to(device)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i: int) -> tuple:\n",
    "        \"\"\"Return tuple from dataset.\"\"\"\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "\n",
    "train_dataset = TreforData(x_train, y_train)\n",
    "val_dataset = TreforData(x_val, y_val)\n",
    "test_dataset = TreforData(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebb505-ae33-4a10-bd6d-8669a746d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testing_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f4b1c-fe92-42a4-9eb6-e5e450e9555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "    ) -> None:\n",
    "        \"\"\"TODO.\"\"\"\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * 24, 24)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"TODO.\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SimpleGRU(1, 5, 1)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869143a-44c3-4af1-8429-16866583f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "plot_train_loss = []\n",
    "plot_val_loss = []\n",
    "\n",
    "\n",
    "def train_one_epoch() -> float:\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + target\n",
    "        inputs, target = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradient\n",
    "        target = target.squeeze(-1)\n",
    "        loss = loss_function(predictions, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100  # loss per 100 batch\n",
    "            # print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0.0\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f42b3-d57c-4b85-9b7b-8d687f5b0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "best_v_loss = sys.maxsize\n",
    "best_model = None\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH {epoch_number + 1}:\")\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    running_v_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            v_inputs, v_target = v_data\n",
    "            v_predictions = model(v_inputs)\n",
    "            v_target = v_target.squeeze(-1)\n",
    "            v_loss = loss_function(v_predictions, v_target)\n",
    "            running_v_loss += v_loss.item()\n",
    "\n",
    "    avg_v_loss = running_v_loss / (i + 1)\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    # print(f'Training : {avg_loss}, Validation : {avg_v_loss}')\n",
    "    plot_train_loss.append(avg_loss)\n",
    "    plot_val_loss.append(avg_v_loss)\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_v_loss < best_v_loss:\n",
    "        best_v_loss = avg_v_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ad36f-b379-43b2-af36-39c57c51c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "predicted = []\n",
    "t_loss = 0\n",
    "num_batches = len(testing_loader)\n",
    "size = len(testing_loader.dataset)\n",
    "with torch.no_grad():\n",
    "    for i, t_data in enumerate(testing_loader):\n",
    "        t_inputs, t_target = t_data\n",
    "        t_predictions = best_model(t_inputs)\n",
    "        predicted.append(t_predictions)\n",
    "        t_loss += loss_function(t_predictions, t_target).item()\n",
    "\n",
    "predicted = torch.cat(predicted, dim=0)\n",
    "t_loss /= num_batches\n",
    "print(f\"Avg loss: {t_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8de414-90df-4acd-84a1-bce9ce8b97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(plot_train_loss, label=\"Training Loss\")\n",
    "plt.plot(plot_val_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47d125-8be3-40bc-9420-9aa4d25bb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalized_y_test = scaler.inverse_transform(y_test.numpy())\n",
    "# denormalized_predicted = scaler.inverse_transform(predicted.numpy())\n",
    "plt.plot(y_test, label=\"Actual Consumption\")\n",
    "plt.plot(predicted, label=\"Predicted Consumption\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
